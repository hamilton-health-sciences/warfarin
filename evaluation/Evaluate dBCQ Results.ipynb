{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2 as pg\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.gridspec as grd\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from matplotlib import ticker\n",
    "from textwrap import wrap\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import feather\n",
    "\n",
    "from scipy.stats import binned_statistic\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import pycountry_convert as pc\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from plotnine import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Best to NOT run this\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import classes defined in separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"/dhi_work/walter/warfarin/warfarin\")\n",
    "from models.smdp_dBCQ import discrete_BCQ, FC_Q\n",
    "from utils.smdp_buffer import SMDPReplayBuffer\n",
    "from helper_functions.model import Model\n",
    "from helper_functions.graphs import Graphs\n",
    "from helper_functions.threshold_model import ThresholdModel\n",
    "from helper_functions.constants import Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load replay buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory locations\n",
    "root_dir = \"../\"\n",
    "save_folder = root_dir + \"output/dBCQ\" \n",
    "buffer_folder = root_dir + \"data/replay_buffers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "num_actions = 7\n",
    "state_dim = 56\n",
    "suffix = \"smdp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "lr = 5e-5\n",
    "bcq_threshold = 0.3\n",
    "iteration = 500000\n",
    "hstates = 64\n",
    "events_batch_size = 0\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_name = f\"actions_{num_actions}_state_{state_dim}_{suffix}\"\n",
    "savename = f\"lr{lr}_bcq{bcq_threshold}_hstates{hstates}_evBatchsize{events_batch_size}_seed{seed}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = {\n",
    "    \"results\": f\"{save_folder}/results/{buffer_name}/{savename}\",\n",
    "    \"models\": f\"{save_folder}/models/{buffer_name}/{savename}\",\n",
    "    \"buffers\": f\"{buffer_folder}/{buffer_name}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buffer = SMDPReplayBuffer(data_path=folder_paths[\"buffers\"] + \"/train_data\", state_dim=state_dim)\n",
    "val_buffer = SMDPReplayBuffer(data_path=folder_paths[\"buffers\"] + \"/val_data\", state_dim=state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buffer.load()\n",
    "val_buffer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"\"\n",
    "baseline = pd.read_feather(f\"../data/clean_data/baseline{suffix}.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if iteration is not None:\n",
    "    try:\n",
    "        model_path = folder_paths[\"models\"] + f\"/checkpoint_{iteration}\"\n",
    "    except Exception as e:\n",
    "        model_path = folder_paths[\"models\"] + f\"/checkpoint\"\n",
    "else:\n",
    "    model_path = folder_paths[\"models\"] + f\"/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = Model.get_model_path(suffix, folder=\"\", lr=lr, iteration=iteration, threshold=bcq_threshold, hidden_states=hstates, events_batch_size=events_batch_size, seed=seed)\n",
    "# model_path = folder_paths[\"models\"]\n",
    "model = Model(model_path, num_actions=num_actions, state_dim=state_dim, hidden_states=hstates, bcq_threshold=bcq_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load replay buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADV_EVENTS = [\"STROKE\", \"HEM_STROKE\", \"MAJOR_BLEED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buffer.data[\"TIME_TO_END\"] = train_buffer.data.groupby('USUBJID_O_NEW').STUDY_WEEK.transform('max') - train_buffer.data.STUDY_WEEK\n",
    "val_buffer.data[\"TIME_TO_END\"] = val_buffer.data.groupby('USUBJID_O_NEW').STUDY_WEEK.transform('max') - val_buffer.data.STUDY_WEEK\n",
    "\n",
    "nonsurvivor_ids = train_buffer.data[(train_buffer.data[ADV_EVENTS].sum(axis=1) > 0)][\"USUBJID_O_NEW\"].unique()\n",
    "train_buffer.data[\"SURVIVOR_FLAG\"] = np.where(train_buffer.data[\"USUBJID_O_NEW\"].isin(nonsurvivor_ids), 0, 1)\n",
    "\n",
    "nonsurvivor_ids = val_buffer.data[(val_buffer.data[ADV_EVENTS].sum(axis=1) > 0)][\"USUBJID_O_NEW\"].unique()\n",
    "val_buffer.data[\"SURVIVOR_FLAG\"] = np.where(val_buffer.data[\"USUBJID_O_NEW\"].isin(nonsurvivor_ids), 0, 1)\n",
    "\n",
    "train_buffer.data['INR_VALUE_ADJ'] = train_buffer.data['INR_VALUE'] * 4 + 0.5\n",
    "train_buffer.data = SMDPReplayBuffer.get_ttr(train_buffer.data, colname='INR_VALUE_ADJ')\n",
    "val_buffer.data['INR_VALUE_ADJ'] = val_buffer.data['INR_VALUE'] * 4 + 0.5\n",
    "val_buffer.data = SMDPReplayBuffer.get_ttr(val_buffer.data, colname='INR_VALUE_ADJ')\n",
    "\n",
    "train_buffer.data['TIME_BELOW_RANGE'] = train_buffer.data.groupby(\"USUBJID_O_NEW\")[\"BELOW_RANGE\"].cumsum() / (train_buffer.data.groupby(\"USUBJID_O_NEW\")[\"BELOW_RANGE\"].cumcount())\n",
    "val_buffer.data['TIME_BELOW_RANGE'] = val_buffer.data.groupby(\"USUBJID_O_NEW\")[\"BELOW_RANGE\"].cumsum() / (val_buffer.data.groupby(\"USUBJID_O_NEW\")[\"BELOW_RANGE\"].cumcount())\n",
    "\n",
    "train_buffer.data['TIME_ABOVE_RANGE'] = train_buffer.data.groupby(\"USUBJID_O_NEW\")[\"ABOVE_RANGE\"].cumsum() / (train_buffer.data.groupby(\"USUBJID_O_NEW\")[\"ABOVE_RANGE\"].cumcount())\n",
    "val_buffer.data['TIME_ABOVE_RANGE'] = val_buffer.data.groupby(\"USUBJID_O_NEW\")[\"ABOVE_RANGE\"].cumsum() / (val_buffer.data.groupby(\"USUBJID_O_NEW\")[\"ABOVE_RANGE\"].cumcount())\n",
    "\n",
    "train_buffer.data[\"INR_BIN\"] = Model.bin_inr(train_buffer.data, colname=\"INR_VALUE_ADJ\", num_bins=5)\n",
    "val_buffer.data[\"INR_BIN\"] = Model.bin_inr(val_buffer.data, colname=\"INR_VALUE_ADJ\", num_bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map back to original data to get the original patient IDs\n",
    "\n",
    "data_dir = \"../data/split_data/\"\n",
    "train_data = feather.read_dataframe(data_dir + \"train_data.feather\")\n",
    "val_data = feather.read_dataframe(data_dir + \"val_data.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mapping = pd.DataFrame(train_data.groupby(\"USUBJID_O_NEW\")[\"SUBJID\"].last()).reset_index()\n",
    "train_buffer.data = train_buffer.data.merge(id_mapping, how=\"left\", on=\"USUBJID_O_NEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mapping = pd.DataFrame(val_data.groupby(\"USUBJID_O_NEW\")[\"SUBJID\"].last()).reset_index()\n",
    "val_buffer.data = val_buffer.data.merge(id_mapping, how=\"left\", on=\"USUBJID_O_NEW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Stats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this state method is used to get the columns of the dataframe.\n",
    "# I'm pretty sure we can get rid of the dependence on \"state method\" since we are already storing the dataframe but ok\n",
    "state_method = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "sample_state1 = deepcopy(train_buffer.state)\n",
    "model.get_model_results(sample_state1, state_method=state_method, num_actions=num_actions)\n",
    "Graphs.plot_heatmap(model.df, axs=axes[0], num_actions=num_actions, return_norm2=False);\n",
    "orig_title = axes[0].title.get_text()\n",
    "axes[0].set_title(f\"Training Data\")\n",
    "\n",
    "sample_state2 = deepcopy(val_buffer.state)\n",
    "model.get_model_results(sample_state2, state_method=state_method, num_actions=num_actions)\n",
    "Graphs.plot_heatmap(model.df, axs=axes[1], num_actions=num_actions, return_norm2=False);\n",
    "axes[1].set_title(f\"Validation Data\")\n",
    "\n",
    "fig.suptitle(orig_title, fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incl_flags = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "buffer = deepcopy(train_buffer)\n",
    "sens, spec = Graphs.get_sens_wrapper(buffer, model, flipped=True, incl_flags=incl_flags)\n",
    "\n",
    "buffer = deepcopy(val_buffer)\n",
    "sens, spec = Graphs.get_sens_wrapper(buffer, model, flipped=True, incl_flags=incl_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buffer = deepcopy(train_buffer)\n",
    "sens, spec = Graphs.get_sens_wrapper(buffer, model, flipped=True, only_direc=True, incl_flags=incl_flags)\n",
    "\n",
    "buffer = deepcopy(val_buffer)\n",
    "sens, spec = Graphs.get_sens_wrapper(buffer, model, flipped=True, only_direc=True, incl_flags=incl_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_to_plot = ['TTR']\n",
    "events_to_plot_ais = ['TTR'] \n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,5), sharex=True, sharey=True)\n",
    "\n",
    "threshold_model = ThresholdModel()\n",
    "ax[0], both_actions1, cut_bins = Graphs.plot_ucurve_new(val_buffer.data, model='threshold', groupby_col=\"ID\", adverse_events=events_to_plot, incl_hist=True, outcome_agg_method=\"last\", axes=ax[0], num_actions=num_actions, use_abs=True, use_qcut=True)\n",
    "ax[0], both_actions2, _ = Graphs.plot_ucurve_new(val_buffer.data, model, groupby_col=\"ID\", adverse_events=events_to_plot_ais, incl_hist=True,  outcome_agg_method=\"last\", axes=ax[0], use_abs=True, cut_bins=cut_bins)\n",
    "# ax[0], _, _ = Graphs.plot_ucurve_new(val_buffer.data, model='naive', groupby_col=\"ID\", adverse_events=events_to_plot_ais, incl_hist=True,  outcome_agg_method=\"last\", axes=ax[0], use_abs=True, cut_bins=cut_bins)\n",
    "# ax[0], _, _ = Graphs.plot_ucurve_new(val_buffer.data, model='random', groupby_col=\"ID\", adverse_events=events_to_plot_ais, incl_hist=True,  outcome_agg_method=\"last\", axes=ax[0], use_abs=True, num_actions=7, cut_bins=cut_bins)\n",
    "\n",
    "\n",
    "# ax[0].set_ylim([0,0.15])\n",
    "ax[0].legend(['RE-LY Algorithm', 'RL Algorithm']) #, 'Do Nothing', 'Random'])\n",
    "ax[0].set_ylabel('TTR')\n",
    "ax[0].set_xlabel('Absolute Difference between Model and Clinician (% Dose Change)')\n",
    "ax[0].set_title(f\"Validation Data: \\n{events_to_plot}\")\n",
    "\n",
    "Graphs.get_ci(both_actions1, 2);\n",
    "Graphs.get_ci(both_actions2, 2);\n",
    "print(f\"percent of patients: {both_actions1[both_actions1['DIFF_ACTIONS_BIN'] == 0].shape[0] / both_actions1.shape[0]:,.2%}, num: {sum(both_actions1['DIFF_ACTIONS_BIN'] == 0)}\")\n",
    "print(f\"percent of patients: {both_actions2[both_actions2['DIFF_ACTIONS_BIN'] == 0].shape[0] / both_actions2.shape[0]:,.2%}, num: {sum(both_actions2['DIFF_ACTIONS_BIN'] == 0)}\")\n",
    "\n",
    "both_actions11, both_actions12 = both_actions1, both_actions2\n",
    "\n",
    "ax[1], both_actions1, cut_bins = Graphs.plot_ucurve_new(train_buffer.data, model='threshold', groupby_col=\"ID\", adverse_events=events_to_plot, incl_hist=True, outcome_agg_method=\"last\", axes=ax[1], num_actions=num_actions, use_abs=True, use_qcut=True)\n",
    "ax[1], both_actions2, _ = Graphs.plot_ucurve_new(train_buffer.data, model, groupby_col=\"ID\", adverse_events=events_to_plot_ais, incl_hist=True,  outcome_agg_method=\"last\", axes=ax[1], use_abs=True, cut_bins=cut_bins)\n",
    "# ax[1], _, _ = Graphs.plot_ucurve_new(train_buffer.data, model='naive', groupby_col=\"ID\", adverse_events=events_to_plot_ais, incl_hist=True,  outcome_agg_method=\"last\", axes=ax[1], use_abs=True, cut_bins=cut_bins)\n",
    "# ax[1], _, _ = Graphs.plot_ucurve_new(train_buffer.data, model='random', groupby_col=\"ID\", adverse_events=events_to_plot_ais, incl_hist=True,  outcome_agg_method=\"last\", axes=ax[1], use_abs=True, num_actions=7, cut_bins=cut_bins)\n",
    "\n",
    "\n",
    "ax[1].legend(['RE-LY Algorithm', 'RL Algorithm']) #, 'Do Nothing', 'Random'])\n",
    "ax[1].set_xlabel('Absolute Difference between Model and Clinician (% Dose Change)')\n",
    "ax[1].set_ylabel('TTR')\n",
    "ax[1].set_title(f\"Training Data: \\n{events_to_plot}\")\n",
    "\n",
    "Graphs.get_ci(both_actions1, 2);\n",
    "Graphs.get_ci(both_actions2, 2);\n",
    "print(f\"percent of patients: {both_actions1[both_actions1['DIFF_ACTIONS_BIN'] == 0].shape[0] / both_actions1.shape[0]:,.2%}, num: {sum(both_actions1['DIFF_ACTIONS_BIN'] == 0)}\")\n",
    "print(f\"percent of patients: {both_actions2[both_actions2['DIFF_ACTIONS_BIN'] == 0].shape[0] / both_actions2.shape[0]:,.2%}, num: {sum(both_actions2['DIFF_ACTIONS_BIN'] == 0)}\")\n",
    "\n",
    "ax[0].set_ylim([0, 1])\n",
    "# ax[0].set_title(\"\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,5), sharex=True)\n",
    "both_actions11['DIFF_ACTIONS_BIN'].hist(alpha=0.5, ax=ax[0])\n",
    "both_actions12['DIFF_ACTIONS_BIN'].hist(alpha=0.5, ax=ax[0])\n",
    "both_actions1['DIFF_ACTIONS_BIN'].hist(alpha=0.5, ax=ax[1])\n",
    "both_actions2['DIFF_ACTIONS_BIN'].hist(alpha=0.5, ax=ax[1])\n",
    "ax[0].set_title(\"Validation Data\")\n",
    "ax[1].set_title(\"Training Data\")\n",
    "ax[0].legend(['RE-LY Algorithm', 'RL Algorithm']) #, 'Do Nothing', 'Random'])\n",
    "ax[1].legend(['RE-LY Algorithm', 'RL Algorithm']) #, 'Do Nothing', 'Random'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate agreement between RELY and RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_to_plot = ['TTR']\n",
    "\n",
    "buffer_data = val_buffer.data\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,5), sharex=True, sharey=True)\n",
    "\n",
    "threshold_model = ThresholdModel()\n",
    "ax[0], both_actions1, cut_bins = Graphs.plot_ucurve_new(buffer_data, model='threshold', groupby_col=\"ID\", adverse_events=events_to_plot, incl_hist=True, outcome_agg_method=\"last\", axes=ax[0], num_actions=num_actions, use_abs=True, use_qcut=True)\n",
    "ax[0], both_actions2, _ = Graphs.plot_ucurve_new(buffer_data, model, groupby_col=\"ID\", adverse_events=events_to_plot_ais, incl_hist=True,  outcome_agg_method=\"last\", axes=ax[0], use_abs=True, cut_bins=cut_bins)\n",
    "\n",
    "plt.close()\n",
    "# ax[0].legend(['RE-LY Algorithm', 'RL Algorithm']) #, 'Do Nothing', 'Random'])\n",
    "# ax[0].set_ylabel('TTR')\n",
    "# ax[0].set_xlabel('Absolute Difference between Model and Clinician (% Dose Change)')\n",
    "# ax[0].set_title(f\"Validation Data: \\n{events_to_plot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_actions1['Model'] = \"RE-LY\"\n",
    "both_actions2['Model'] = 'RL'\n",
    "temp_df = pd.concat([both_actions1, both_actions2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.boxplot(data=temp_df, x=\"DIFF_ACTIONS_BIN\", y=\"AGREE\", hue='Model')\n",
    "sns.violinplot(data=temp_df, x=\"DIFF_ACTIONS_BIN\", y=\"ADV_EVENTS\", hue='Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RE-LY: -- {both_actions1[both_actions1['DIFF_ACTIONS_BIN'] == 0].shape[0]} entries at x=0\")\n",
    "print(f\"RL: ----- {both_actions2[both_actions2['DIFF_ACTIONS_BIN'] == 0].shape[0]} entries at x=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_actions = both_actions1.merge(both_actions2, left_index=True, right_index=True)\n",
    "both_zero = merged_actions[(merged_actions['DIFF_ACTIONS_BIN_x'] == 0) & (merged_actions['DIFF_ACTIONS_BIN_y'] == 0)]\n",
    "print(f\"{both_zero.shape[0]} trajectories where both RE-LY and RL are at x=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mapping = buffer_data.groupby('USUBJID_O_NEW')['SUBJID'].last()\n",
    "subset_baseline = baseline[baseline['SUBJID'].isin(buffer_data['SUBJID'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_zero = both_zero.merge(id_mapping, how=\"left\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_ids = both_zero['SUBJID'].unique()\n",
    "print(f\"{len(pat_ids)} unique patients at x=0\")\n",
    "zero_patients = baseline[baseline['SUBJID'].isin(pat_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_rely = merged_actions[(merged_actions['DIFF_ACTIONS_BIN_x'] == 0) & (merged_actions['DIFF_ACTIONS_BIN_y'] != 0)]\n",
    "only_rl = merged_actions[(merged_actions['DIFF_ACTIONS_BIN_y'] == 0) & (merged_actions['DIFF_ACTIONS_BIN_x'] != 0)]\n",
    "print(f\"{only_rely.shape[0]} trajectories in RE-LY x=0\")\n",
    "print(f\"{only_rl.shape[0]} trajectories in RL x=0\")\n",
    "\n",
    "only_rely = only_rely.merge(id_mapping, how=\"left\", left_index=True, right_index=True)\n",
    "only_rl = only_rl.merge(id_mapping, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "only_rely = baseline[baseline['SUBJID'].isin(only_rely['SUBJID'].unique())]\n",
    "only_rl = baseline[baseline['SUBJID'].isin(only_rl['SUBJID'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{only_rely['SUBJID'].nunique()} unique patients in RE-LY x=0\")\n",
    "print(f\"{only_rl['SUBJID'].nunique()} unique patients in RL x=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_rely['Patient Group'] = 'x=0 for RELY only'\n",
    "only_rl['Patient Group'] = 'x=0 for RL only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_patients['Patient Group'] = \"x=0 for both RELY and RL\"\n",
    "\n",
    "other_patients = baseline[~baseline['SUBJID'].isin(pat_ids)]\n",
    "other_patients['Patient Group'] = 'Other Patients'\n",
    "\n",
    "baseline['Patient Group'] = 'Entire Dataset'\n",
    "\n",
    "all_patients = pd.concat([zero_patients, other_patients, only_rely, only_rl, baseline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_data = pd.concat([subs0, subs1, subs2])\n",
    "x, y, hue, title = \"REGION\", \"% Dataset\", \"Patient Group\", \"Dist of Patient Demographic\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "Graphs.create_hist(all_patients, x, y, hue, title=title)\n",
    "plt.xticks(rotation=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, hue, title = \"SEX\", \"% Dataset\", \"Patient Group\", \"Dist of Patient Demographic\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "Graphs.create_hist(all_patients, x, y, hue, title=title)\n",
    "plt.xticks(rotation=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, hue, title = \"TRIAL\", \"% Dataset\", \"Patient Group\", \"Dist of Patient Demographic\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "Graphs.create_hist(all_patients, x, y, hue, title=title)\n",
    "plt.xticks(rotation=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, hue, title = \"HX_MI\", \"% Dataset\", \"Patient Group\", \"Dist of Patient Demographic\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "Graphs.create_hist(all_patients, x, y, hue, title=title)\n",
    "plt.xticks(rotation=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patients['AGE_DEIDENTIFIED'] = all_patients['AGE_DEIDENTIFIED'].apply(lambda x: 90 if x == \">89\" else float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_bins = [-0.001, 50, 60, 65, 70, 75, 80, 91]\n",
    "cut_labels = [\"<=50\", \"(50, 60]\", \"(60, 65]\", \"(65, 70]\", \"(70, 75]\", \"(75, 80]\", \">80\"]\n",
    "\n",
    "all_patients['AGE_BIN'] = pd.cut(all_patients['AGE_DEIDENTIFIED'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "counts = all_patients['AGE_BIN'].value_counts(normalize=True)\n",
    "counts = counts.reindex(cut_labels)\n",
    "\n",
    "plt.bar(x=counts.index, height=counts.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, hue, title = \"AGE_BIN\", \"% Dataset\", \"Patient Group\", \"Dist of Patient Demographic\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "Graphs.create_hist(all_patients, x, y, hue, title=title)\n",
    "plt.xticks(rotation=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance across demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colname = \"CONTINENT_EAST ASIA\"\n",
    "Graphs.get_action_heatmap_by_feature(train_buffer, val_buffer, colname, num_actions, state_method, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colname = \"CONTINENT_NORTH AMERICA\"\n",
    "Graphs.get_action_heatmap_by_feature(train_buffer, val_buffer, colname, num_actions, state_method, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colname = \"CONTINENT_EASTERN EUROPE\"\n",
    "Graphs.get_action_heatmap_by_feature(train_buffer, val_buffer, colname, num_actions, state_method, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_buffer.data[\"CONTINENT\"] = val_buffer.data[[x for x in val_buffer.data.columns if \"CONTINENT\" in x]].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_buffer.data[\"CONTINENT\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(val_buffer.data, x=\"TTR\", hue=\"CONTINENT\", kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_to_plot = ['TTR']\n",
    "buffer_data = deepcopy(val_buffer.data)\n",
    "buffer_data = buffer_data.drop(columns=[\"SUBJID\", \"CONTINENT\"])\n",
    "\n",
    "colname1 = \"CONTINENT_EAST ASIA\"\n",
    "colname2 = \"CONTINENT_EASTERN EUROPE\"\n",
    "\n",
    "Graphs.get_ucurve_by_demographic_feature(buffer_data, model, events_to_plot, colname1, colname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_to_plot = ['TTR']\n",
    "buffer_data = deepcopy(val_buffer.data)\n",
    "buffer_data = buffer_data.drop(columns=[\"SUBJID\", \"CONTINENT\"])\n",
    "\n",
    "colname1 = \"CONTINENT_EAST ASIA\"\n",
    "colname2 = \"CONTINENT_NORTH AMERICA\"\n",
    "\n",
    "Graphs.get_ucurve_by_demographic_feature(buffer_data, model, events_to_plot, colname1, colname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_to_plot = ['TTR']\n",
    "buffer_data = deepcopy(train_buffer.data)\n",
    "\n",
    "colname1 = \"CONTINENT_EAST ASIA\"\n",
    "colname2 = \"CONTINENT_NORTH AMERICA\"\n",
    "\n",
    "Graphs.get_ucurve_by_demographic_feature(buffer_data, model, events_to_plot, colname1, colname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_to_plot = ['TTR']\n",
    "buffer_data = deepcopy(train_buffer.data)\n",
    "\n",
    "colname1 = \"CONTINENT_EAST ASIA\"\n",
    "colname2 = \"CONTINENT_EASTERN EUROPE\"\n",
    "\n",
    "Graphs.get_ucurve_by_demographic_feature(buffer_data, model, events_to_plot, colname1, colname2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between agreement and TTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_to_plot = ['TTR']\n",
    "events_to_plot_ais = ['TTR'] \n",
    "\n",
    "if \"CONTINENT\" in val_buffer.data:\n",
    "    val_buffer.data = val_buffer.data.drop(columns=\"CONTINENT\")\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = plt.gca()\n",
    "\n",
    "threshold_model = ThresholdModel()\n",
    "ax, both_actions1, cut_bins = Graphs.plot_ucurve_new(val_buffer.data, model='threshold', groupby_col=\"ID\", adverse_events=events_to_plot, incl_hist=True, outcome_agg_method=\"last\", axes=ax, num_actions=num_actions, use_abs=True, use_qcut=True)\n",
    "ax, both_actions2, _ = Graphs.plot_ucurve_new(val_buffer.data, model, groupby_col=\"ID\", adverse_events=events_to_plot_ais, incl_hist=True,  outcome_agg_method=\"last\", axes=ax, use_abs=True, cut_bins=cut_bins)\n",
    "\n",
    "ax.legend(['RE-LY Algorithm', 'RL Algorithm']) #, 'Do Nothing', 'Random'])\n",
    "ax.set_ylabel('TTR')\n",
    "ax.set_xlabel('Absolute Difference between Model and Clinician (% Dose Change)')\n",
    "ax.set_title(f\"Validation Data: \\n{events_to_plot}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = both_actions1['AGREE'].values.reshape(-1, 1)\n",
    "y = both_actions1['ADV_EVENTS'].values\n",
    "weight = both_actions1['TRAJ_LENGTH'].values\n",
    "\n",
    "reg = LinearRegression().fit(X,y)\n",
    "print(reg.coef_)\n",
    "print(reg.score(X, y))\n",
    "print(reg.predict(np.array([100, 50]).reshape(-1, 1)))\n",
    "\n",
    "reg = LinearRegression().fit(X,y,weight)\n",
    "print(reg.coef_)\n",
    "print(reg.score(X, y))\n",
    "print(reg.predict(np.array([100, 50]).reshape(-1, 1)))\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = both_actions2['AGREE'].values.reshape(-1, 1)\n",
    "y = both_actions2['ADV_EVENTS'].values\n",
    "\n",
    "print()\n",
    "reg = LinearRegression().fit(X,y)\n",
    "print(reg.coef_)\n",
    "print(reg.score(X, y))\n",
    "print(reg.predict(np.array([100, 50]).reshape(-1, 1)))\n",
    "\n",
    "reg = LinearRegression().fit(X,y,weight)\n",
    "print(reg.coef_)\n",
    "print(reg.score(X, y))\n",
    "print(reg.predict(np.array([100, 50]).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_actions = deepcopy(both_actions1)\n",
    "\n",
    "both_actions['ID'] = both_actions.index.values\n",
    "both_actions['SUBJID'] = both_actions['ID'].apply(lambda x: float(x.split(\".\")[0] + x.split(\".\")[-1])).values\n",
    "both_actions = both_actions.merge(baseline[['SUBJID', 'REGION']], on ='SUBJID', how='left')\n",
    "both_actions_agg = both_actions.groupby('REGION').mean()\n",
    "\n",
    "ax = sns.regplot(data=both_actions_agg[both_actions_agg['AGREE'] <= 1], x=\"AGREE\", y=\"ADV_EVENTS\", ci=None)\n",
    "ax.set_xlabel(\"% Agreement with Model\")\n",
    "ax.set_ylabel(\"TTR (%)\")\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(1))\n",
    "ax.xaxis.set_major_formatter(PercentFormatter(1))\n",
    "# ax.set_xlim([0.449, 0.474])\n",
    "ax.set_xlim([0.43, 0.57])\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = both_actions_agg['AGREE'].values.reshape(-1, 1)\n",
    "y = both_actions_agg['ADV_EVENTS'].values\n",
    "weight = both_actions_agg['TRAJ_LENGTH'].values\n",
    "\n",
    "reg = LinearRegression().fit(X,y)\n",
    "print(reg.coef_)\n",
    "print(reg.score(X, y))\n",
    "print(reg.predict(np.array([100, 50]).reshape(-1, 1)))\n",
    "\n",
    "reg = LinearRegression().fit(X,y,weight)\n",
    "print(reg.coef_)\n",
    "print(reg.score(X, y))\n",
    "print(reg.predict(np.array([100, 50]).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_actions = deepcopy(both_actions2)\n",
    "\n",
    "both_actions['ID'] = both_actions.index.values\n",
    "both_actions['SUBJID'] = both_actions['ID'].apply(lambda x: float(x.split(\".\")[0] + x.split(\".\")[-1])).values\n",
    "both_actions = both_actions.merge(baseline[['SUBJID', 'REGION']], on ='SUBJID', how='left')\n",
    "both_actions_agg = both_actions.groupby('REGION').mean()\n",
    "\n",
    "ax = sns.regplot(data=both_actions_agg[both_actions_agg['AGREE'] <= 1], x=\"AGREE\", y=\"ADV_EVENTS\", ci=None)\n",
    "ax.set_xlabel(\"% Agreement with Model\")\n",
    "ax.set_ylabel(\"TTR (%)\")\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(1))\n",
    "ax.xaxis.set_major_formatter(PercentFormatter(1))\n",
    "# ax.set_xlim([0.449, 0.474])\n",
    "ax.set_xlim([0.43, 0.57])\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = both_actions_agg['AGREE'].values.reshape(-1, 1)\n",
    "y = both_actions_agg['ADV_EVENTS'].values\n",
    "weight = both_actions_agg['TRAJ_LENGTH'].values\n",
    "\n",
    "reg = LinearRegression().fit(X,y)\n",
    "print(reg.coef_)\n",
    "print(reg.score(X, y))\n",
    "print(reg.predict(np.array([100, 50]).reshape(-1, 1)))\n",
    "\n",
    "reg = LinearRegression().fit(X,y,weight)\n",
    "print(reg.coef_)\n",
    "print(reg.score(X, y))\n",
    "print(reg.predict(np.array([100, 50]).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(data=both_actions1, x=\"AGREE\", y=\"ADV_EVENTS\")\n",
    "ax.set_xlabel(\"% Agreement with Model\")\n",
    "ax.set_ylabel(\"TTR (%)\")\n",
    "ax.set_title(\"Threshold Model\")\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(1))\n",
    "ax.xaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "X = both_actions11['AGREE'].values.reshape(-1, 1)\n",
    "y = both_actions11['ADV_EVENTS'].values\n",
    "weight = both_actions11['TRAJ_LENGTH'].values\n",
    "\n",
    "reg = LinearRegression().fit(X,y)\n",
    "print(reg.coef_)\n",
    "print(reg.score(X, y))\n",
    "print(reg.predict(np.array([100, 50]).reshape(-1, 1)))\n",
    "\n",
    "reg = LinearRegression().fit(X,y,weight)\n",
    "print(reg.coef_)\n",
    "print(reg.score(X, y))\n",
    "print(reg.predict(np.array([100, 50]).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(data=both_actions2, x=\"AGREE\", y=\"ADV_EVENTS\")\n",
    "ax.set_xlabel(\"% Agreement with Model\")\n",
    "ax.set_ylabel(\"TTR (%)\")\n",
    "ax.set_title(\"dBCQ Model\")\n",
    "\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(1))\n",
    "ax.xaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "\n",
    "X = both_actions12['AGREE'].values.reshape(-1, 1)\n",
    "y = both_actions12['ADV_EVENTS'].values\n",
    "weight = both_actions12['TRAJ_LENGTH'].values\n",
    "\n",
    "reg = LinearRegression().fit(X,y)\n",
    "print(reg.coef_)\n",
    "print(reg.score(X, y))\n",
    "print(reg.predict(np.array([100, 50]).reshape(-1, 1)))\n",
    "\n",
    "reg = LinearRegression().fit(X,y,weight)\n",
    "print(reg.coef_)\n",
    "print(reg.score(X, y))\n",
    "print(reg.predict(np.array([100, 50]).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistency(Q):\n",
    "    \"\"\"\n",
    "    Measures whether a given action distribution is \"consistent\" ordinally.\n",
    "    \"\"\"\n",
    "    D = np.diff(Q, axis=-1)\n",
    "\n",
    "    # Impute leading zeros\n",
    "    idxs = (D != 0.).argmax(axis=-1)\n",
    "    for n, idx in enumerate(idxs):\n",
    "        D[n, :idx] = 2.\n",
    "    # Impute trailing zeros\n",
    "    idxs = D.shape[1] - (np.flip(D, axis=-1) != 0.).argmax(axis=-1)\n",
    "    for n, idx in enumerate(idxs):\n",
    "        D[n, idx:] = -2.\n",
    "\n",
    "    c = np.sign(-D)\n",
    "    c_all = np.all(np.sort(c, axis=-1) == c, axis=-1)\n",
    "\n",
    "    return c_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_state2 = deepcopy(val_buffer.state)\n",
    "q, imt, _ = model.model(torch.FloatTensor(sample_state2).to(\"cpu\"))\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imt = imt.exp()\n",
    "imt = (imt / imt.max(1, keepdim=True)[0] > bcq_threshold).float()\n",
    "# Use large negative number to mask actions from argmax\n",
    "# return ((imt * q + (1. - imt) * -1e8).argmax(1)).astype(int)\n",
    "# probs = pd.DataFrame([x for x in (imt * q + (1. - imt) * -1e8).to(\"cpu\").detach().numpy()])\n",
    "probs = pd.DataFrame(model.get_model_actions(sample_state2, return_prob=True, use_threshold=True))\n",
    "probs = probs.replace(-1e8, np.nan)\n",
    "Q = probs.ffill(axis=1).bfill(axis=1)\n",
    "sum(consistency(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_state_df = pd.DataFrame(val_buffer.state, columns=Constants.state_cols_mapping[state_method])\n",
    "# val_state_df['consistency'] = consistency(Q)\n",
    "X_full = val_state_df\n",
    "y = consistency(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "rfe = RFE(logreg, 20)\n",
    "rfe = rfe.fit(X_full, y)\n",
    "# print(rfe.support_)\n",
    "# print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_cols = Constants.state_cols_mapping[state_method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_full.loc[:, np.array(state_cols)[rfe.support_]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largest positive drivers\n",
    "pos_feats = pd.Series(result.params).nlargest(5)\n",
    "\n",
    "indx = np.array(pos_feats.index)\n",
    "for i in indx:\n",
    "    print(f\"{i}: \\t {pos_feats[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largest negative drivers \n",
    "neg_feats = pd.Series(result.params).nsmallest(5)\n",
    "\n",
    "indx = np.array(neg_feats.index)\n",
    "for i in indx:\n",
    "    print(f\"{i}: \\t {neg_feats[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = pd.DataFrame(q.detach().numpy())\n",
    "diff_probs = pd.DataFrame(q.diff(axis=1).detach().numpy())\n",
    "\n",
    "probs['mean'] = probs[[0,1,2,3,4,5,6]].mean(axis=1)\n",
    "probs['action'] = probs.idxmax(axis=1)\n",
    "probs['action_prob'] = probs.max(axis=1)\n",
    "probs['dev_action'] = probs['action_prob'] - probs['mean']\n",
    "probs['dev_action_pct'] = probs['dev_action'] / probs['mean']\n",
    "\n",
    "plt.figure(figsize=(11,6))\n",
    "sns.boxplot(data=probs, x='action', y='dev_action_pct')\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.ylabel(\"% Deviation of Action Prob From Mean\")\n",
    "plt.xlabel(\"Action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imt = imt.exp()\n",
    "imt = (imt / imt.max(1, keepdim=True)[0] > bcq_threshold).float()\n",
    "# Use large negative number to mask actions from argmax\n",
    "# return ((imt * q + (1. - imt) * -1e8).argmax(1)).astype(int)\n",
    "# probs = pd.DataFrame([x for x in (imt * q + (1. - imt) * -1e8).to(\"cpu\").detach().numpy()])\n",
    "probs = pd.DataFrame(model.get_model_actions(sample_state2, return_prob=True, use_threshold=True))\n",
    "probs = probs.replace(-1e8, np.nan)\n",
    "probs['mean'] = probs[[0,1,2,3,4,5,6]].mean(axis=1)\n",
    "probs['action'] = probs.idxmax(axis=1)\n",
    "probs['action_prob'] = probs.max(axis=1)\n",
    "probs['dev_action'] = probs['action_prob'] - probs['mean']\n",
    "probs['dev_action_pct'] = probs['dev_action'] / probs['mean']\n",
    "\n",
    "plt.figure(figsize=(11,6))\n",
    "sns.boxplot(data=probs, x='action', y='dev_action_pct')\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.ylabel(\"% Deviation of Action Prob From Mean\")\n",
    "plt.xlabel(\"Action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[(~probs[[0,1,2]].isnull().all(axis=1)) & (~probs[[4,5,6]].isnull().all(axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.melt(diff_probs, var_name=\"Index of Action\", value_name=\"Difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.boxplot(data=df_long, x='Index of Action', y='Difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PCA to reduce the dimensionality of the sate, and then use tSNE to vsualize the clusters. Investigate the actions in each cluster, and investigate how similar these clusters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_state = deepcopy(val_buffer.data)\n",
    "sample_state = sample_state[~sample_state['WEIGHT'].isnull()]\n",
    "\n",
    "sample_state.loc[:, \"INR_1\"] = sample_state.groupby('USUBJID_O_NEW')['INR_VALUE'].shift(1).fillna(\n",
    "    sample_state[\"INR_VALUE\"])\n",
    "sample_state.loc[:, \"INR_2\"] = sample_state.groupby('USUBJID_O_NEW')['INR_VALUE'].shift(2).fillna(\n",
    "    sample_state[\"INR_1\"])\n",
    "sample_state.loc[:, \"INR_3\"] = sample_state.groupby('USUBJID_O_NEW')['INR_VALUE'].shift(3).fillna(\n",
    "    sample_state[\"INR_2\"])\n",
    "sample_state.loc[:, \"INR_4\"] = sample_state.groupby('USUBJID_O_NEW')['INR_VALUE'].shift(4).fillna(\n",
    "    sample_state[\"INR_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = deepcopy(val_buffer.state)\n",
    "cols = sample_state[Constants.state_cols_mapping[state_method]]\n",
    "cols = [x for x in cols if x not in [\"INR_1\", \"INR_2\", \"INR_3\", \"INR_4\"]]\n",
    "X = sample_state[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = [x for x in sample_state.columns if \"CONTINENT_\" in x]\n",
    "sample_state['CONTINENT'] = sample_state[cont_cols].idxmax(axis=1).apply(lambda x: x.split(\"_\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = [x for x in sample_state.columns if \"AGE_BIN_\" in x]\n",
    "sample_state['AGE_BIN'] = sample_state[cont_cols].idxmax(axis=1).apply(lambda x: x.split(\"_\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PCA directly and visualize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "pca_result = pca.fit_transform(X)\n",
    "df = pd.DataFrame({})\n",
    "ind = Constants.state_cols_mapping[state_method].index('CONTINENT_EAST ASIA')\n",
    "# df['y'] = X[:,ind]\n",
    "df['y'] = sample_state['CONTINENT']\n",
    "\n",
    "df['pca-one'] = pca_result[:,0]\n",
    "df['pca-two'] = pca_result[:,1] \n",
    "df['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", df['y'].nunique()),\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "pca_result = pca.fit_transform(X)\n",
    "df = pd.DataFrame({})\n",
    "ind = Constants.state_cols_mapping[state_method].index('CONTINENT_EAST ASIA')\n",
    "# df['y'] = X[:,ind]\n",
    "df['y'] = sample_state['AGE_BIN']\n",
    "\n",
    "df['pca-one'] = pca_result[:,0]\n",
    "df['pca-two'] = pca_result[:,1] \n",
    "df['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", df['y'].nunique()),\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "pca_result = pca.fit_transform(X)\n",
    "df = pd.DataFrame({})\n",
    "df['y'] = sample_state['SEX']\n",
    "\n",
    "df['pca-one'] = pca_result[:,0]\n",
    "df['pca-two'] = pca_result[:,1] \n",
    "df['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", df['y'].nunique()),\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA and k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(pca_result)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans_pca.fit(pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_state['Cluster'] = kmeans_pca.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "pca_result = pca.fit_transform(X)\n",
    "df = pd.DataFrame({})\n",
    "ind = Constants.state_cols_mapping[state_method].index('SEX')\n",
    "# df['y'] = X[:,ind]\n",
    "df['y'] = sample_state['Cluster']\n",
    "\n",
    "df['pca-one'] = pca_result[:,0]\n",
    "df['pca-two'] = pca_result[:,1] \n",
    "df['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", df['y'].nunique()),\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "sample_state['weight'] = np.where(sample_state[Constants.neg_reward_events].sum(axis=1) > 0, 10, 0)\n",
    "\n",
    "pca_result = pca.fit_transform(X)\n",
    "df = pd.DataFrame({})\n",
    "df['y'] = sample_state['STROKE']\n",
    "df['weight'] = sample_state['weight']\n",
    "\n",
    "df['pca-one'] = pca_result[:,0]\n",
    "df['pca-two'] = pca_result[:,1] \n",
    "df['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", df['y'].nunique()),\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    hue=\"y\",\n",
    "    color=\"\",\n",
    "    data=df[df['y']==1],\n",
    "    legend=\"full\",\n",
    "    alpha=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "trans_X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300).fit_transform(trans_X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = pd.DataFrame({})\n",
    "ind = Constants.state_cols_mapping[state_method].index('SEX')\n",
    "df_subset['y'] = sample_state['CONTINENT']\n",
    "df_subset['tsne-2d-one'] = X_embedded[:,0]\n",
    "df_subset['tsne-2d-two'] = X_embedded[:,1]\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", df_subset['y'].nunique()),\n",
    "    data=df_subset,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.9, conda)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
